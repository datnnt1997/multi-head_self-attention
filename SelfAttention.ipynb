{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7lDraqJMavJ",
        "colab_type": "text"
      },
      "source": [
        "#**Multi-Head Attention**\n",
        "***by Đắt Ngô***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QusC6QouL-Cx",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMiQj5gNWRr4",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image01.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29LNoMkHX_tQ",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image02.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkPqOW52YF5b",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image03.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4JSD7dpYLE7",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image04.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fmxG6OJVFUb",
        "colab_type": "text"
      },
      "source": [
        "# Mục mới"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwd20OtaMJaG",
        "colab_type": "text"
      },
      "source": [
        "# Implement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx0or3QwdfKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9_zoFP9uvVZ",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "      def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config[\"hidden_size\"] % config[\"num_of_attention_heads\"] == 0, \"The hidden size is not a multiple of the number of attention heads\"\n",
        "\n",
        "        self.num_attention_heads = config['num_of_attention_heads']\n",
        "        self.attention_head_size = int(config['hidden_size'] / config['num_of_attention_heads'])\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config['hidden_size'], self.all_head_size)\n",
        "        self.key = nn.Linear(config['hidden_size'], self.all_head_size)\n",
        "        self.value = nn.Linear(config['hidden_size'], self.all_head_size)\n",
        "\n",
        "        self.dense = nn.Linear(config['hidden_size'], config['hidden_size'])\n",
        "\n",
        "      def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "      def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)                             # [Batch_size x Seq_length x Hidden_size]\n",
        "        mixed_key_layer = self.key(hidden_states)                                 # [Batch_size x Seq_length x Hidden_size]\n",
        "        mixed_value_layer = self.value(hidden_states)                             # [Batch_size x Seq_length x Hidden_size]\n",
        "        \n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)                # [Batch_size x Num_of_heads x Seq_length x Head_size]\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)                    # [Batch_size x Num_of_heads x Seq_length x Head_size]\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)                # [Batch_size x Num_of_heads x Seq_length x Head_size]\n",
        "\n",
        "        \n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # [Batch_size x Num_of_heads x Seq_length x Seq_length]\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # [Batch_size x Num_of_heads x Seq_length x Seq_length]\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)                    # [Batch_size x Num_of_heads x Seq_length x Seq_length]\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)                # [Batch_size x Num_of_heads x Seq_length x Head_size]\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()            # [Batch_size x Seq_length x Num_of_heads x Head_size]\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) # [Batch_size x Seq_length x Hidden_size]\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)              # [Batch_size x Seq_length x Hidden_size]\n",
        "        \n",
        "        output =  self.dense(context_layer)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESinjyx7HoZx",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image05.png?raw=true)\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52o5EX2NIrVh",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image06.png?raw=true)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01yop3GTJNX5",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image07.png?raw=true)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZH0YX_PJyZ7",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image08.png?raw=true)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pCuHPrBKKme",
        "colab_type": "text"
      },
      "source": [
        "![Image Error](https://github.com/dat821168/multi-head_self-attention/blob/master/images/image09.png?raw=true)\n",
        " \n",
        "    output =  self.dense(context_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umgyYKpd4xoD",
        "colab_type": "text"
      },
      "source": [
        "Khởi tạo model self_Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qa7cprKKY3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    \"num_of_attention_heads\": 2,\n",
        "    \"hidden_size\": 4\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SamrN5YW1oHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "eb338ca9-b363-4dc6-e614-4c1254ed0ffe"
      },
      "source": [
        "selfattn = BertSelfAttention(config)\n",
        "print(selfattn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertSelfAttention(\n",
            "  (query): Linear(in_features=4, out_features=4, bias=True)\n",
            "  (key): Linear(in_features=4, out_features=4, bias=True)\n",
            "  (value): Linear(in_features=4, out_features=4, bias=True)\n",
            "  (dense): Linear(in_features=4, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX9wzFXjKaZc",
        "colab_type": "text"
      },
      "source": [
        "Khởi tạo ngẫu nhiên embedding đầu vào"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWui4t4Z176-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "062d4701-0af9-410a-b2fa-4c3da7cffb1f"
      },
      "source": [
        "embed_rand = torch.rand((1,3,4))\n",
        "print(f\"Embed Shape: {embed_rand.shape}\")\n",
        "print(f\"Embed Values:\\n{embed_rand}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embed Shape: torch.Size([1, 3, 4])\n",
            "Embed Values:\n",
            "tensor([[[0.0552, 0.1801, 0.1834, 0.2938],\n",
            "         [0.3367, 0.1240, 0.7171, 0.6137],\n",
            "         [0.7021, 0.6584, 0.2185, 0.7412]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xhCV4N4LSDP",
        "colab_type": "text"
      },
      "source": [
        "# Forward input embedding với SelfAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou7Kou8GLdO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "206d6fe4-1171-4d1a-bf5d-6d448681a0be"
      },
      "source": [
        "output = selfattn(embed_rand)\n",
        "print(f\"Output Shape: {output.shape}\")\n",
        "print(f\"Output Values:\\n{output}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Shape: torch.Size([1, 3, 4])\n",
            "Output Values:\n",
            "tensor([[[ 0.1697,  0.4565,  0.4477, -0.0096],\n",
            "         [ 0.1689,  0.4558,  0.4477, -0.0084],\n",
            "         [ 0.1706,  0.4573,  0.4455, -0.0104]]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}